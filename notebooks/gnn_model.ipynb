{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/koksal/.conda/envs/caghan4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from itertools import tee\n",
    "from functools import lru_cache\n",
    "\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "from torch_geometric.transforms import BaseTransform, Compose, FaceToEdge\n",
    "from torch_geometric.data import Data, InMemoryDataset, extract_zip, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"\"\"Iterate over all pairs of consecutive items in a list.\n",
    "    Notes\n",
    "    -----\n",
    "        [s0, s1, s2, s3, ...] -> (s0,s1), (s1,s2), (s2, s3), ...\n",
    "    \"\"\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def get_conv_layers(channels: list, conv: MessagePassing, conv_params: dict):\n",
    "    \"\"\"Define convolution layers with specified in and out channels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels: list\n",
    "        List of integers specifying the size of the convolution channels.\n",
    "    conv: MessagePassing\n",
    "        Convolution layer.\n",
    "    conv_params: dict\n",
    "        Dictionary specifying convolution parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of convolutions with the specified channels.\n",
    "    \"\"\"\n",
    "    conv_layers = [\n",
    "        conv(in_ch, out_ch, **conv_params) for in_ch, out_ch in pairwise(channels)\n",
    "    ]\n",
    "    return conv_layers\n",
    "\n",
    "def get_mlp_layers(channels: list, activation, output_activation=nn.Identity):\n",
    "    \"\"\"Define basic multilayered perceptron network.\"\"\"\n",
    "    layers = []\n",
    "    *intermediate_layer_definitions, final_layer_definition = pairwise(channels)\n",
    "\n",
    "    for in_ch, out_ch in intermediate_layer_definitions:\n",
    "        intermediate_layer = nn.Linear(in_ch, out_ch)\n",
    "        layers += [intermediate_layer, activation()]\n",
    "\n",
    "    layers += [nn.Linear(*final_layer_definition), output_activation()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSteeredConvolution(MessagePassing):\n",
    "    \"\"\"Implementation of feature steered convolutions.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Verma, Nitika, Edmond Boyer, and Jakob Verbeek.\n",
    "       \"Feastnet: Feature-steered graph convolutions for 3d shape analysis.\"\n",
    "       Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        num_heads: int,\n",
    "        ensure_trans_invar: bool = True,\n",
    "        bias: bool = True,\n",
    "        with_self_loops: bool = True,\n",
    "    ):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.with_self_loops = with_self_loops\n",
    "\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=in_channels,\n",
    "            out_features=out_channels * num_heads,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.u = torch.nn.Linear(\n",
    "            in_features=in_channels,\n",
    "            out_features=num_heads,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.c = torch.nn.Parameter(torch.Tensor(num_heads))\n",
    "\n",
    "        if not ensure_trans_invar:\n",
    "            self.v = torch.nn.Linear(\n",
    "                in_features=in_channels,\n",
    "                out_features=num_heads,\n",
    "                bias=False,\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter(\"v\", None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialization of tuneable network parameters.\"\"\"\n",
    "        torch.nn.init.uniform_(self.linear.weight)\n",
    "        torch.nn.init.uniform_(self.u.weight)\n",
    "        torch.nn.init.normal_(self.c, mean=0.0, std=0.1)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.normal_(self.bias, mean=0.0, std=0.1)\n",
    "        if self.v is not None:\n",
    "            torch.nn.init.uniform_(self.v.weight)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"Forward pass through a feature steered convolution layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.tensor [|V|, in_features]\n",
    "            Input feature matrix, where each row describes\n",
    "            the input feature descriptor of a node in the graph.\n",
    "        edge_index: torch.tensor [2, E]\n",
    "            Edge matrix capturing the graph's\n",
    "            edge structure, where each row describes an edge\n",
    "            between two nodes in the graph.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor [|V|, out_features]\n",
    "            Output feature matrix, where each row corresponds\n",
    "            to the updated feature descriptor of a node in the graph.\n",
    "        \"\"\"\n",
    "        if self.with_self_loops:\n",
    "            edge_index, _ = remove_self_loops(edge_index)\n",
    "            edge_index, _ = add_self_loops(edge_index=edge_index, num_nodes=x.shape[0])\n",
    "\n",
    "        out = self.propagate(edge_index, x=x)\n",
    "        return out if self.bias is None else out + self.bias\n",
    "\n",
    "    def _compute_attention_weights(self, x_i, x_j):\n",
    "        \"\"\"Computation of attention weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_i: torch.tensor [|E|, in_feature]\n",
    "            Matrix of feature embeddings for all central nodes,\n",
    "            collecting neighboring information to update its embedding.\n",
    "        x_j: torch.tensor [|E|, in_features]\n",
    "            Matrix of feature embeddings for all neighboring nodes\n",
    "            passing their messages to the central node along\n",
    "            their respective edge.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor [|E|, M]\n",
    "            Matrix of attention scores, where each row captures\n",
    "            the attention weights of transformed node in the graph.\n",
    "        \"\"\"\n",
    "        if x_j.shape[-1] != self.in_channels:\n",
    "            raise ValueError(\n",
    "                f\"Expected input features with {self.in_channels} channels.\"\n",
    "                f\" Instead received features with {x_j.shape[-1]} channels.\"\n",
    "            )\n",
    "        if self.v is None:\n",
    "            attention_logits = self.u(x_i - x_j) + self.c\n",
    "        else:\n",
    "            attention_logits = self.u(x_i) + self.b(x_j) + self.c\n",
    "        return F.softmax(attention_logits, dim=1)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        \"\"\"Message computation for all nodes in the graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_i: torch.tensor [|E|, in_feature]\n",
    "            Matrix of feature embeddings for all central nodes,\n",
    "            collecting neighboring information to update its embedding.\n",
    "        x_j: torch.tensor [|E|, in_features]\n",
    "            Matrix of feature embeddings for all neighboring nodes\n",
    "            passing their messages to the central node along\n",
    "            their respective edge.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor [|E|, out_features]\n",
    "            Matrix of updated feature embeddings for\n",
    "            all nodes in the graph.\n",
    "        \"\"\"\n",
    "        attention_weights = self._compute_attention_weights(x_i, x_j)\n",
    "        x_j = self.linear(x_j).view(-1, self.num_heads, self.out_channels)\n",
    "        return (attention_weights.view(-1, self.num_heads, 1) * x_j).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeatureEncoder(torch.nn.Module):\n",
    "    \"\"\"Graph neural network consisting of stacked graph convolutions.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        conv_channels,\n",
    "        num_heads,\n",
    "        apply_batch_norm: int = True,\n",
    "        ensure_trans_invar: bool = True,\n",
    "        bias: bool = True,\n",
    "        with_self_loops: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_params = dict(\n",
    "            num_heads=num_heads,\n",
    "            ensure_trans_invar=ensure_trans_invar,\n",
    "            bias=bias,\n",
    "            with_self_loops=with_self_loops,\n",
    "        )\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "\n",
    "        *first_conv_channels, final_conv_channel = conv_channels\n",
    "        conv_layers = get_conv_layers(\n",
    "            channels=[in_features] + conv_channels,\n",
    "            conv=FeatureSteeredConvolution,\n",
    "            conv_params=conv_params,\n",
    "        )\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "\n",
    "        self.batch_layers = [None for _ in first_conv_channels]\n",
    "        if apply_batch_norm:\n",
    "            self.batch_layers = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(channel) for channel in first_conv_channels]\n",
    "            )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        *first_conv_layers, final_conv_layer = self.conv_layers\n",
    "        for conv_layer, batch_layer in zip(first_conv_layers, self.batch_layers):\n",
    "            x = conv_layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            if batch_layer is not None:\n",
    "                x = batch_layer(x)\n",
    "        return final_conv_layer(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshSeg(torch.nn.Module):\n",
    "    \"\"\"Mesh segmentation network.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        encoder_features,\n",
    "        conv_channels,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        apply_batch_norm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_encoder = get_mlp_layers(\n",
    "            channels=[in_features] + encoder_channels,\n",
    "            activation=nn.ReLU,\n",
    "        )\n",
    "        self.gnn = GraphFeatureEncoder(\n",
    "            in_features=encoder_features,\n",
    "            conv_channels=conv_channels,\n",
    "            num_heads=num_heads,\n",
    "            apply_batch_norm=apply_batch_norm,\n",
    "        )\n",
    "        *_, final_conv_channel = conv_channels\n",
    "\n",
    "        self.final_projection = get_mlp_layers(\n",
    "            [final_conv_channel] + decoder_channels + [num_classes],\n",
    "            activation=nn.ReLU,\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.input_encoder(x)\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x =  self.final_projection(x)\n",
    "\n",
    "        # Pool\n",
    "        return torch.mean(x,dim=0)#.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_data, optimizer, loss_fn, device):\n",
    "    \"\"\"Train network on training dataset.\"\"\"\n",
    "    net.train()\n",
    "    cumulative_loss = 0.0\n",
    "    for data in train_data:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = net(data)\n",
    "        #print('Out shape ', out.shape)\n",
    "        #print('data y shape', data.y.shape)\n",
    "        loss = loss_fn(out, data.y.float())\n",
    "        loss.backward()\n",
    "        cumulative_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return cumulative_loss / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, gt_seg_labels):\n",
    "    \"\"\"Compute accuracy of predicted segmentation labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: [|V|, num_classes]\n",
    "        Soft predictions of segmentation labels.\n",
    "    gt_seg_labels: [|V|]\n",
    "        Ground truth segmentations labels.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of predicted segmentation labels.    \n",
    "    \"\"\"\n",
    "    predicted_seg_labels = predictions.argmax(dim=-1, keepdim=True)\n",
    "    if predicted_seg_labels.shape != gt_seg_labels.shape:\n",
    "        raise ValueError(\"Expected Shapes to be equivalent\")\n",
    "    correct_assignments = (predicted_seg_labels == gt_seg_labels).sum()\n",
    "    num_assignemnts = predicted_seg_labels.shape[0]\n",
    "    return float(correct_assignments / num_assignemnts)\n",
    "\n",
    "\n",
    "def evaluate_performance(dataset, net, device):\n",
    "    \"\"\"Evaluate network performance on given dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: DataLoader\n",
    "        Dataset on which the network is evaluated on.\n",
    "    net: torch.nn.Module\n",
    "        Trained network.\n",
    "    device: str\n",
    "        Device on which the network is located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float:\n",
    "        Mean accuracy of the network's prediction on\n",
    "        the provided dataset.\n",
    "    \"\"\"\n",
    "    prediction_accuracies = []\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        predictions = net(data)\n",
    "        prediction_accuracies.append(accuracy(predictions, data.y))\n",
    "    return sum(prediction_accuracies) / len(prediction_accuracies)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(net, train_data, test_data, device):\n",
    "    net.eval()\n",
    "    train_acc = evaluate_performance(train_data, net, device)\n",
    "    test_acc = evaluate_performance(test_data, net, device)\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(\n",
    "    in_features=3,\n",
    "    encoder_features=16,\n",
    "    conv_channels=[32, 64, 128, 64],\n",
    "    encoder_channels=[16],\n",
    "    decoder_channels=[32],\n",
    "    num_classes=1,\n",
    "    num_heads=12,\n",
    "    apply_batch_norm=True,\n",
    ")\n",
    "\n",
    "net = MeshSeg(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeshSeg(\n",
       "  (input_encoder): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
       "    (1): Identity()\n",
       "  )\n",
       "  (gnn): GraphFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): FeatureSteeredConvolution(16, 32)\n",
       "      (1): FeatureSteeredConvolution(32, 64)\n",
       "      (2): FeatureSteeredConvolution(64, 128)\n",
       "      (3): FeatureSteeredConvolution(128, 64)\n",
       "    )\n",
       "    (batch_layers): ModuleList(\n",
       "      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (final_projection): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (3): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks',\n",
       " '/u/home/koksal/.conda/envs/caghan4/lib/python39.zip',\n",
       " '/u/home/koksal/.conda/envs/caghan4/lib/python3.9',\n",
       " '/u/home/koksal/.conda/envs/caghan4/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/u/home/koksal/.conda/envs/caghan4/lib/python3.9/site-packages',\n",
       " '/u/home/koksal/organ-mesh-registration-and-property-prediction/']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/u/home/koksal/organ-mesh-registration-and-property-prediction/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.organs_dataset import OrganMeshDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/vol/chameleon/projects/mesh_gnn/organ_meshes'\n",
    "basic_feat_path = '/vol/chameleon/projects/mesh_gnn/basic_features.csv'\n",
    "bridge_path = '/vol/chameleon/projects/mesh_gnn/Bridge_eids_60520_87802.csv'\n",
    "split_path = '/u/home/koksal/organ-mesh-registration-and-property-prediction/data/'\n",
    "\n",
    "\n",
    "train_dataset = OrganMeshDataset(root, basic_feat_path, bridge_path, num_samples = 1000, mode='train', split_path=split_path )\n",
    "val_dataset = OrganMeshDataset(root, basic_feat_path, bridge_path,  num_samples = 100, mode='val', split_path=split_path )\n",
    "test_dataset = OrganMeshDataset(root, basic_feat_path, bridge_path,  num_samples =100, mode='test', split_path=split_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/koksal/.conda/envs/caghan4/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset,  shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "best_test_acc = 0.0\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:13<?, ?Epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epochs), unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepochs:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tepochs:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         train_loss \u001b[39m=\u001b[39m train(net, train_loader, optimizer, loss_fn, device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         train_acc, test_acc \u001b[39m=\u001b[39m test(net, train_loader, test_loader, device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         tepochs\u001b[39m.\u001b[39mset_postfix(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m             train_loss\u001b[39m=\u001b[39mtrain_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             train_accuracy\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m train_acc,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m             test_accuracy\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m test_acc,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         )\n",
      "\u001b[1;32m/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_data, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     cumulative_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B131.159.110.3/u/home/koksal/organ-mesh-registration-and-property-prediction/notebooks/gnn_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cumulative_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_data)\n",
      "File \u001b[0;32m~/.conda/envs/caghan4/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/caghan4/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.conda/envs/caghan4/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/caghan4/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.conda/envs/caghan4/lib/python3.9/site-packages/torch/optim/adam.py:397\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     step \u001b[39m=\u001b[39m step_t\u001b[39m.\u001b[39mitem()\n\u001b[0;32m--> 397\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m step\n\u001b[1;32m    398\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n\u001b[1;32m    400\u001b[0m     step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(range(num_epochs), unit=\"Epoch\") as tepochs:\n",
    "    for epoch in tepochs:\n",
    "        train_loss = train(net, train_loader, optimizer, loss_fn, device)\n",
    "        train_acc, test_acc = test(net, train_loader, test_loader, device)\n",
    "        \n",
    "        tepochs.set_postfix(\n",
    "            train_loss=train_loss,\n",
    "            train_accuracy=100 * train_acc,\n",
    "            test_accuracy=100 * test_acc,\n",
    "        )\n",
    "        sleep(0.1)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            #torch.save(net.state_dict(), \"/content/checkpoint_best_colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('caghan4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7396ef363109521e03a1a0d63e78822cb454baa0fd8fa9ea26684fd7d4e3122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
